{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from MLModels import LinearRegression, LogisticRegression, UnivariantLinearRegression, LDA, PCA, KNN, BayesClassifier\n",
    "from NeuralNetworks import *\n",
    "import pandas as pd\n",
    "# import numpy as np\n",
    "# nnfs.init()\n",
    "from CNN import *\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss:  3.6251979825350213\n",
      "Y_hat:  4.109356342638097\n"
     ]
    }
   ],
   "source": [
    "reg = LinearRegression()\n",
    "\n",
    "x = np.array([[1,2,3],[0,1,1],[1,1,0],[1,0,1]]) # Dummy X_train\n",
    "y = np.array([2.4,5.6,2.3,2.7]) # Dummy Y_train\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(x,y)\n",
    "print('Loss: ',model.loss) # Loss after final iteration\n",
    "\n",
    "print(\"Y_hat: \",model.predict(np.array([1,2,3]))) # Dummy X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "Specifically, the KL divergence measures a very similar quantity to cross-entropy. It measures the average number of extra bits required to represent a message with Q instead of P, not the total number of bits.\n",
    "\n",
    "If two probability distributions are the same, then the cross-entropy between them will be the entropy of the distribution.\n",
    "\n",
    "[All about Cross Entropy, Entropy, Kl-Divergence, Log Loss](https://machinelearningmastery.com/cross-entropy-for-machine-learning/)\n",
    "\n",
    "**Log loss = negative log-likelihood (under a Bernoulli probability distribution)**\n",
    "\n",
    "`Cross_Entropy(Y,Y_hat) = Log_Loss(Y,Y_hat) = Entropy(Y) + KL_Divergence(Y,Y_hat)`\n",
    "\n",
    "`Cross_Entropy(Y,Y_hat) != Cross_Entropy(Y_hat,Y)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0852763569399853\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "cla = LogisticRegression()\n",
    "y = np.array([1,0,0,1])\n",
    "cla.fit(x,y)\n",
    "\n",
    "print(cla.loss)\n",
    "\n",
    "prob, cla = cla.predict(np.array([1,2,3]))\n",
    "print(cla)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.26470281, -0.4800266 , -0.12770602,  0.0241682 ],\n",
       "       [-2.08096115,  0.67413356, -0.23460885,  0.10300677],\n",
       "       [-2.36422905,  0.34190802,  0.04420148,  0.02837705]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import datasets\n",
    "from sklearn.decomposition import PCA as PCA_sk\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data  # we only take the first two features.\n",
    "X = StandardScaler().fit_transform(X)\n",
    "\n",
    "pca = PCA(4)\n",
    "pca.fit(X,transform=True)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-2.26470281,  0.4800266 , -0.12770602, -0.0241682 ],\n",
       "       [-2.08096115, -0.67413356, -0.23460885, -0.10300677],\n",
       "       [-2.36422905, -0.34190802,  0.04420148, -0.02837705]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_sk = PCA_sk(4,random_state=13)\n",
    "pca_sk.fit_transform(X)[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.07919271, -1.40598311,  1.46075404,  0.61898115]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test = np.array([[-0.4,1.6,-0.3,1.3]])\n",
    "pca.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.07919271,  1.40598311,  1.46075404, -0.61898115]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_sk.transform(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn = KNN(k=3)\n",
    "x = np.array([[1,2,3],[3,4,5],[0,1,0]]) # Dummy X features\n",
    "y = np.array([0,1,0])\n",
    "x_test = np.array([[2,3,4]])\n",
    "\n",
    "knn.fit(x,y)\n",
    "knn.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 1, 0, 1, 2, 3]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.array([0,1,2,0,0,1,2,3])\n",
    "X = np.random.rand(8,10)\n",
    "bayes = BayesClassifier()\n",
    "bayes.fit(X,y)\n",
    "bayes.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.25      , 0.33333333, 0.41666667])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = [1,1,1,1,0,2,2,2,2,2,0,0]\n",
    "np.bincount(y)/len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33333337 0.33333325 0.33333337]\n",
      " [0.33333334 0.33333334 0.33333334]\n",
      " [0.33333334 0.33333334 0.33333334]]\n",
      "loss: 1.0986105\n"
     ]
    }
   ],
   "source": [
    "X, y = spiral_data(samples = 100 , classes = 3)\n",
    "dense1  = Dense(2, 3 )\n",
    "activation1 = ReLu()\n",
    "dense2 = Dense(3 , 3 )\n",
    "activation2 = Softmax()\n",
    "loss_function = CategoricalCrossEntropy() \n",
    "dense1.forward(X)\n",
    "activation1.forward(dense1.output)\n",
    "dense2.forward(activation1.output)\n",
    "activation2.forward(dense2.output)\n",
    "print(activation2.output[:5])\n",
    "loss = loss_function.average_loss(activation2.output, y)\n",
    "print('loss:', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f95b5aad2b0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (5.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "np.random.seed(1)\n",
    "x = np.random.randn(4, 3, 3, 3)\n",
    "x_pad = add_padding(x,(4,1),0)\n",
    "\n",
    "\n",
    "fig, axarr = plt.subplots(1, 2)\n",
    "axarr[0].set_title('x')\n",
    "axarr[0].imshow(x[0,:,:,0])\n",
    "axarr[1].set_title('x_pad')\n",
    "axarr[1].imshow(x_pad[0,:,:,0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Whole UNet is divided in 3 parts: Endoder -> BottleNeck -> Decoder. There are skip connections between 'Nth' level of Encoder with Nth level of Decoder.\n",
    "\n",
    "There is 1 basic entity called \"Convolution\" Block which has 3*3 Convolution (or Transposed Convolution during Upsampling) -> ReLu -> BatchNorm\n",
    "Then there is Maxpooling\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ConvolutionBlock(nn.Module):\n",
    "    '''\n",
    "    The basic Convolution Block Which Will have Convolution -> RelU -> Convolution -> RelU\n",
    "    '''\n",
    "    def __init__(self, input_features, out_features):\n",
    "        '''\n",
    "        args:\n",
    "            batch_norm was introduced after UNET so they did not know if it existed. Might be useful\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(input_features, out_features, kernel_size = 3, padding= 0), # padding is 0 by default, 1 means the input width, height == out width, height\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_features, out_features, kernel_size = 3, padding = 0),\n",
    "            nn.ReLU(),\n",
    "            )\n",
    "\n",
    "    def forward(self, feature_map_x):\n",
    "        '''\n",
    "        feature_map_x could be the image itself or the\n",
    "        '''\n",
    "        return self.network(feature_map_x)\n",
    "\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    '''\n",
    "    '''\n",
    "    def __init__(self, image_channels:int = 3, blockwise_features = [64, 128, 256, 512]):\n",
    "        '''\n",
    "        In UNET, the features start at 64 and keeps getting twice the size of the previous one till it reached BottleNeck\n",
    "        args:\n",
    "            image_channels: Channels in the Input Image. Typically it is any of the 1 or 3 (rarely 4)\n",
    "            blockwise_features = Each block has it's own input and output features. it means first ConV block will output 64 features, second 128 and so on\n",
    "        '''\n",
    "        super().__init__()\n",
    "        repeat = len(blockwise_features) # how many layers we need to add len of blockwise_features == len of out_features\n",
    "\n",
    "        self.layers = nn.ModuleList()\n",
    "        \n",
    "        for i in range(repeat):\n",
    "            if i == 0:\n",
    "                in_filters = image_channels\n",
    "                out_filters = blockwise_features[0]\n",
    "            else:\n",
    "                in_filters = blockwise_features[i-1]\n",
    "                out_filters = blockwise_features[i]\n",
    "            \n",
    "            self.layers.append(ConvolutionBlock(in_filters, out_filters))\n",
    "\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size = 2, stride = 2)  # Since There is No gradient for Maxpooling, You can instantiate a single layer for the whole operation\n",
    "        # https://datascience.stackexchange.com/questions/11699/backprop-through-max-pooling-layers\n",
    "        \n",
    "    \n",
    "    def forward(self, feature_map_x):\n",
    "        skip_connections = [] # i_th level of features from Encoder will be conatenated with i_th level of decoder before applying CNN\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            feature_map_x = layer(feature_map_x)\n",
    "            skip_connections.append(feature_map_x)\n",
    "            feature_map_x = self.maxpool(feature_map_x) # Use Max Pooling AFTER storing the Skip connections\n",
    "\n",
    "        return feature_map_x, skip_connections\n",
    "\n",
    "    \n",
    "class BottleNeck(nn.Module):\n",
    "    '''\n",
    "    ConvolutionBlock without Max Pooling\n",
    "    '''\n",
    "    def __init__(self, input_features = 512, output_features = 1024):\n",
    "        super().__init__()\n",
    "        self.layer = ConvolutionBlock(input_features, output_features)\n",
    "\n",
    "        \n",
    "    def forward(self, feature_map_x):\n",
    "        return self.layer(feature_map_x)\n",
    "        \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    '''\n",
    "    '''\n",
    "    def __init__(self, blockwise_features = [512, 256, 128, 64]):\n",
    "        '''\n",
    "        Do exactly opposite of Encoder\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.upsample_layers = nn.ModuleList()\n",
    "        self.conv_layers = nn.ModuleList()\n",
    "        \n",
    "        for i, feature in enumerate(blockwise_features):\n",
    "\n",
    "            self.upsample_layers.append(nn.ConvTranspose2d(in_channels = feature*2, out_channels = feature, kernel_size = 2, stride = 2))  # Takes in 1024-> 512, takes 512->254 ......\n",
    "\n",
    "            self.conv_layers.append(ConvolutionBlock(input_features=feature*2, out_features= feature)) # After Concatinating (512 + 512-> 1024), Use double Conv block\n",
    "        \n",
    "    \n",
    "    def forward(self, feature_map_x, skip_connections):\n",
    "        '''\n",
    "        Steps go as:\n",
    "        1. Upsample\n",
    "        2. Concat Skip Connection\n",
    "        3. Apply ConvolutionBlock\n",
    "        '''\n",
    "\n",
    "        for i, layer in enumerate(self.conv_layers): # 4 levels, 4 skip connections, 4 upsampling, 4 Double Conv Block\n",
    "            skip_feature = skip_connections[-i-1]\n",
    "            feature_map_x = self.upsample_layers[i](feature_map_x) # step 1\n",
    "\n",
    "            if skip_feature.shape[-1] != feature_map_x.shape[-1]: # The part of COPY CROP to make the dimensions equal\n",
    "                pad = (skip_feature.shape[-1] - feature_map_x.shape[-1]) // 2 # Half padding on each side, \n",
    "                feature_map_x = nn.functional.pad(feature_map_x, pad = [pad,pad], )\n",
    "\n",
    "\n",
    "            feature_map_x = torch.cat((skip_feature, feature_map_x), dim = 1) # step 2, Concatinating along Channels or Features dimensions given [N,C,W,H]\n",
    "            feature_map_x = self.conv_layers[i](feature_map_x)\n",
    "\n",
    "        return feature_map_x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 1. Got 64 and 56 in dimension 2 (The offending index is 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-8fb325bd5385>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mdec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDecoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mfeats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# feat = nn.ConvTranspose2d(1024, 512,2,2)(feat)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/contrib_py37/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    887\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    888\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 889\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    890\u001b[0m         for hook in itertools.chain(\n\u001b[1;32m    891\u001b[0m                 \u001b[0m_global_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-50-050a77ea49a3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, feature_map_x, skip_connections)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m             \u001b[0mfeature_map_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mskip_feature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeature_map_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# step 2, Concatinating along Channels or Features dimensions given [N,C,W,H]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m             \u001b[0mfeature_map_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeature_map_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 1. Got 64 and 56 in dimension 2 (The offending index is 1)"
     ]
    }
   ],
   "source": [
    "image = torch.randn(1,3, 572,572)\n",
    "enc = Encoder(3, )\n",
    "feat, skip = enc(image)\n",
    "bot = BottleNeck()\n",
    "feat = bot(feat)\n",
    "\n",
    "dec = Decoder()\n",
    "feats = dec(feat, skip)\n",
    "\n",
    "# feat = nn.ConvTranspose2d(1024, 512,2,2)(feat)\n",
    "# pad = (skip[-1].shape[-1] - feat.shape[-1]) // 2\n",
    "# feat = nn.functional.pad(feat, [pad,pad,pad, pad])\n",
    "# feat = torch.cat([skip[-1],feat],dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            [(None, 512, 256, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 518, 262, 3)  0           input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 256, 128, 64) 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 256, 128, 64) 256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 256, 128, 64) 0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 258, 130, 64) 0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 128, 64, 64)  0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 128, 64, 64)  4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 128, 64, 64)  256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 128, 64, 64)  0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 128, 64, 64)  36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 128, 64, 64)  256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 128, 64, 64)  0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 128, 64, 256) 16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 128, 64, 256) 16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 128, 64, 256) 1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 128, 64, 256) 1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 128, 64, 256) 0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 128, 64, 256) 0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 128, 64, 64)  16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 128, 64, 64)  256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 128, 64, 64)  0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 128, 64, 64)  36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 128, 64, 64)  256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 128, 64, 64)  0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 128, 64, 256) 16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 128, 64, 256) 1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 128, 64, 256) 0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 128, 64, 256) 0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 128, 64, 64)  16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 128, 64, 64)  256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 128, 64, 64)  0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 128, 64, 64)  36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 128, 64, 64)  256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 128, 64, 64)  0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 128, 64, 256) 16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 128, 64, 256) 1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 128, 64, 256) 0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 128, 64, 256) 0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 64, 32, 128)  32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 64, 32, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 64, 32, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 64, 32, 128)  147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 64, 32, 128)  512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 64, 32, 128)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 64, 32, 512)  131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 64, 32, 512)  66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 64, 32, 512)  2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 64, 32, 512)  2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 64, 32, 512)  0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 64, 32, 512)  0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 64, 32, 128)  65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 64, 32, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 64, 32, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 64, 32, 128)  147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 64, 32, 128)  512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 64, 32, 128)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 64, 32, 512)  66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 64, 32, 512)  2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 64, 32, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 64, 32, 512)  0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 64, 32, 128)  65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 64, 32, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 64, 32, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 64, 32, 128)  147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 64, 32, 128)  512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 64, 32, 128)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 64, 32, 512)  66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 64, 32, 512)  2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 64, 32, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 64, 32, 512)  0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 64, 32, 128)  65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 64, 32, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 64, 32, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 64, 32, 128)  147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 64, 32, 128)  512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 64, 32, 128)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 64, 32, 512)  66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 64, 32, 512)  2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 64, 32, 512)  0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 64, 32, 512)  0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 32, 16, 256)  131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 32, 16, 256)  1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 32, 16, 256)  0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 32, 16, 256)  590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 32, 16, 256)  1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 32, 16, 256)  0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 32, 16, 1024) 525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 32, 16, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 32, 16, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 32, 16, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 32, 16, 1024) 0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 32, 16, 1024) 0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 32, 16, 256)  262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 32, 16, 256)  1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 32, 16, 256)  0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 32, 16, 256)  590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 32, 16, 256)  1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 32, 16, 256)  0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 32, 16, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 32, 16, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 32, 16, 1024) 0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 32, 16, 1024) 0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 32, 16, 256)  262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 32, 16, 256)  1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 32, 16, 256)  0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 32, 16, 256)  590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 32, 16, 256)  1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 32, 16, 256)  0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 32, 16, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 32, 16, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 32, 16, 1024) 0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 32, 16, 1024) 0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 32, 16, 256)  262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 32, 16, 256)  1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 32, 16, 256)  0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 32, 16, 256)  590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 32, 16, 256)  1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 32, 16, 256)  0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 32, 16, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 32, 16, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 32, 16, 1024) 0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 32, 16, 1024) 0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 32, 16, 256)  262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 32, 16, 256)  1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 32, 16, 256)  0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 32, 16, 256)  590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 32, 16, 256)  1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 32, 16, 256)  0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 32, 16, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 32, 16, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 32, 16, 1024) 0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 32, 16, 1024) 0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 32, 16, 256)  262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 32, 16, 256)  1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 32, 16, 256)  0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 32, 16, 256)  590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 32, 16, 256)  1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 32, 16, 256)  0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "average_pooling2d_6 (AveragePoo (None, 1, 1, 256)    0           conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 1, 1, 256)    65792       average_pooling2d_6[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 1, 1, 256)    1024        conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 32, 16, 256)  65536       conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 32, 16, 256)  589824      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 32, 16, 256)  589824      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 32, 16, 256)  589824      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_56 (ReLU)                 (None, 1, 1, 256)    0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 32, 16, 256)  1024        conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 32, 16, 256)  1024        conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 32, 16, 256)  1024        conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 32, 16, 256)  1024        conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_17 (UpSampling2D) (None, 32, 16, 256)  0           re_lu_56[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_52 (ReLU)                 (None, 32, 16, 256)  0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_53 (ReLU)                 (None, 32, 16, 256)  0           batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_54 (ReLU)                 (None, 32, 16, 256)  0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_55 (ReLU)                 (None, 32, 16, 256)  0           batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_12 (Concatenate)    (None, 32, 16, 1280) 0           up_sampling2d_17[0][0]           \n",
      "                                                                 re_lu_52[0][0]                   \n",
      "                                                                 re_lu_53[0][0]                   \n",
      "                                                                 re_lu_54[0][0]                   \n",
      "                                                                 re_lu_55[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 32, 16, 256)  327680      concatenate_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 32, 16, 256)  1024        conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 128, 64, 48)  3072        conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_57 (ReLU)                 (None, 32, 16, 256)  0           batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 128, 64, 48)  192         conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_18 (UpSampling2D) (None, 128, 64, 256) 0           re_lu_57[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_58 (ReLU)                 (None, 128, 64, 48)  0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_13 (Concatenate)    (None, 128, 64, 304) 0           up_sampling2d_18[0][0]           \n",
      "                                                                 re_lu_58[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 128, 64, 256) 700416      concatenate_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 128, 64, 256) 1024        conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_59 (ReLU)                 (None, 128, 64, 256) 0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 128, 64, 256) 589824      re_lu_59[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 128, 64, 256) 1024        conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "re_lu_60 (ReLU)                 (None, 128, 64, 256) 0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "up_sampling2d_19 (UpSampling2D) (None, 512, 256, 256 0           re_lu_60[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 512, 256, 20) 5140        up_sampling2d_19[0][0]           \n",
      "==================================================================================================\n",
      "Total params: 11,857,236\n",
      "Trainable params: 11,824,500\n",
      "Non-trainable params: 32,736\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def DilatedConvolutionBlock(input_features, kernel_size = 3, dilation_rate = 1, filters = 256, padding = 'same', use_bias = False):\n",
    "    '''\n",
    "    Takes in some features and perform the folloring operations: Dilated (Atrous) Contolution -> BatchNorm -> ReLu\n",
    "    To read more about \"SAME\" Padding: read https://stackoverflow.com/questions/68035443/what-does-padding-same-exactly-mean-in-tensorflow-conv2d-is-it-minimum-paddin\n",
    "    '''\n",
    "    x = Convolution2D(filters, kernel_size, dilation_rate = dilation_rate, use_bias = use_bias, padding = padding)(input_features)\n",
    "    x = BatchNormalization()(x)\n",
    "    return ReLU()(x) # tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def Perform_ASPP(input_features):\n",
    "    '''\n",
    "    Get features From Backbone and Create Atrous Spatial Pyramid Pooling. \n",
    "    The features are first bilinearly upsampled by a factor 4, and then concatenated with the corresponding low-level features from the network backbone that have the same spatial resolution.\n",
    "\n",
    "    Pay attention to the \"ENCODER\" part here: https://miro.medium.com/max/1037/1*2mYfKnsX1IqCCSItxpXSGA.png In this image, DCNN is any pre-trained architecture features\n",
    "    '''\n",
    "    width, height = input_features.shape[1], input_features.shape[2] # given Channels Last format\n",
    "\n",
    "    one_cross_one = DilatedConvolutionBlock(input_features = input_features, kernel_size = 1, dilation_rate = 1,)\n",
    "    rate_6 = DilatedConvolutionBlock(input_features = input_features, dilation_rate = 6,) # (default) 3x3 convolution with dilation rate of 6\n",
    "    rate_12 = DilatedConvolutionBlock(input_features = input_features, dilation_rate = 12,)\n",
    "    rate_18 = DilatedConvolutionBlock(input_features = input_features, dilation_rate = 18,)\n",
    "\n",
    "    pooling_part = AveragePooling2D(pool_size = (width, height))(input_features)\n",
    "    pooling_part = DilatedConvolutionBlock(pooling_part, kernel_size = 1, use_bias = True)\n",
    "    pooling_part = UpSampling2D(size=(width, height), interpolation=\"bilinear\")(pooling_part)\n",
    "    # We could use Transposed Convolution for this, for difference, see: https://stackoverflow.com/questions/53654310/what-is-the-difference-between-upsampling2d-and-conv2dtranspose-functions-in-ker\n",
    "\n",
    "    concatenated_fetures = Concatenate(axis = -1)([pooling_part, one_cross_one, rate_6, rate_12, rate_18]) # Concatenate all features\n",
    "    return DilatedConvolutionBlock(concatenated_fetures, kernel_size = 1, )\n",
    "    \n",
    "\n",
    "def EncoderPart(backbone, right_layer_name, down_layer_name):\n",
    "    '''\n",
    "    The dotted ENCODER part in the image: https://miro.medium.com/max/1037/1*2mYfKnsX1IqCCSItxpXSGA.png\n",
    "    Takes an input as Image and generates two outputs:\n",
    "    1. Features extracted from down_layer_name is directly sent to the Decoder (where 1x1 convolution will be performed later)\n",
    "    2. Right Path: Feature which are computed from the ASPP\n",
    "\n",
    "    args:\n",
    "        backbone: Pre Trained Model\n",
    "        right_layer_name: Layer name whose output will be used for ASPP. You can try with different layers and their outputs\n",
    "        down_layer_name: Layer name whose output will be sent directly to the Decoder\n",
    "    '''\n",
    "    features = backbone.get_layer(right_layer_name).output # this is the layer whose output they have used as Features extraction. You could also use: backbone.layers[142].output\n",
    "    right_path = Perform_ASPP(features)\n",
    "    return backbone.get_layer(down_layer_name).output, right_path\n",
    "\n",
    "\n",
    "def DecoderPart(down_path_features, right_path_features, image_shape):\n",
    "    '''\n",
    "    The dotted DECODER part in the image: https://miro.medium.com/max/1037/1*2mYfKnsX1IqCCSItxpXSGA.png\n",
    "    Takes two inputs (down_path_features, right_path_features: See the DocString of Encoder to see what they are) and return an output\n",
    "    '''\n",
    "    first_upsampling_by_4 = UpSampling2D(size = (4, 4), interpolation = 'bilinear')(right_path_features)\n",
    "\n",
    "    one_cross_one = DilatedConvolutionBlock(down_path_features, kernel_size = 1, dilation_rate = 1, filters = 48)\n",
    "\n",
    "    concat = Concatenate(axis = -1)([first_upsampling_by_4, one_cross_one])\n",
    "\n",
    "    x = DilatedConvolutionBlock(concat, kernel_size = 3, dilation_rate = 1) # default parameters are used\n",
    "    x = DilatedConvolutionBlock(x, kernel_size = 3, dilation_rate = 1)\n",
    "\n",
    "    second_upsampling_by_4 = UpSampling2D(size = (4,4), interpolation = 'bilinear')(x)\n",
    "    return second_upsampling_by_4\n",
    "\n",
    "\n",
    "def DeepLabV3Plus(num_classes, backbone = ResNet50, image_shape = (512,512,3), right_layer_name = \"conv4_block6_2_relu\", down_layer_name = \"conv2_block3_2_relu\"):\n",
    "    '''\n",
    "    Build DeepLabV3+ Architecture Model. Following Steps are Done:\n",
    "    1. Fetch a pre trained Model and instantiate it\n",
    "    2. Extract 2 different features (direct feature from one layer, features from another layer where ASPP is performed) from the encoder part\n",
    "    3. Pass those features to Decoder to get final features\n",
    "    4. Pass those final features to 1x1 convolution so that makss can be generated from the images\n",
    "\n",
    "    args:\n",
    "        num_classes: Total number of classes that have to be predicted\n",
    "    '''\n",
    "    assert (not image_shape[0] % 4) and (not image_shape[1] % 4), \"Image Height and Width must be divisible by 4\" \n",
    "    backbone = backbone(input_tensor = Input(image_shape),include_top=False, weights=\"imagenet\") # Step 1\n",
    "\n",
    "    down_path_features, right_path_features = EncoderPart(backbone, right_layer_name, down_layer_name) # step 2\n",
    "\n",
    "    decoder_features = DecoderPart(down_path_features, right_path_features, image_shape) # step 3\n",
    "\n",
    "    output = Conv2D(num_classes, kernel_size = 1, padding = 'same')(decoder_features) # step 4\n",
    "    model = Model(inputs = backbone.input, outputs = output)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = DeepLabV3Plus(num_classes = 20, backbone = ResNet50, image_shape = (512,256,3)) \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128.0"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "512/ 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64.0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "256 /4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
